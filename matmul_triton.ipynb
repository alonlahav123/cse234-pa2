{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "E-mNhUjQuxNM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N9lmLw8cuxNN"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eUMlpjFJuxNO"
      },
      "outputs": [],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lBNGYaejuxNO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PA2 Part 2: MatMul+Relu+Add Fused Optimization.\n",
        "The kernel uses several optimization techniques:\n",
        "\n",
        "  1. Shared memory tiling.\n",
        "  2. Register tiling.\n",
        "  3. Cooperative fetching.\n",
        "  4. Operator Fusion\n",
        "  5. Write cache / epilogue fusion.\n",
        "\n",
        "Fill in the missing parts (marked with TODO).\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 128  # Tile size in the M dimension.\n",
        "BLOCK_N = 128 # Tile size in the N dimension.\n",
        "BLOCK_K = 32 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # GROUP_M = 8\n",
        "    # # Program ID\n",
        "    # pid = tl.program_id(axis=0)\n",
        "    # # Number of program ids along the M axis\n",
        "    # num_pid_m = tl.cdiv(M, BLOCK_M)\n",
        "    # # Number of programs ids along the N axis\n",
        "    # num_pid_n = tl.cdiv(N, BLOCK_N)\n",
        "    # # Number of programs in group\n",
        "    # num_pid_in_group = GROUP_M * num_pid_n\n",
        "    # # Id of the group this program is in\n",
        "    # group_id = pid // num_pid_in_group\n",
        "    # # Row-id of the first program in the group\n",
        "    # first_pid_m = group_id * GROUP_M\n",
        "    # # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n",
        "    # group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n",
        "    # # *Within groups*, programs are ordered in a column-major order\n",
        "    # # Row-id of the program in the *launch grid*\n",
        "    # pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
        "    # # Col-id of the program in the *launch grid*\n",
        "    # pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Tile: Assignment\n",
        "    #\n",
        "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
        "    # Compute the starting indices (m_start, n_start) for this tile.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
        "    m_id = tl.program_id(axis=0)  # get index within M dimension\n",
        "    n_id = tl.program_id(axis=1)  # get index within N dimension\n",
        "\n",
        "    m_start = m_id * BLOCK_M  # index of kernel * size of block within dimension M\n",
        "    n_start = n_id * BLOCK_N  # index of kernel * size of block within dimension M\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Register Tiling\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), tl.float16)  # size: (block_m, block_n)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
        "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
        "    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n",
        "    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n",
        "    # -------------------------------------------------------------------------\n",
        "    offs_m = (m_start + tl.arange(0, BLOCK_M)) % M  # size: (block_m)\n",
        "    offs_n = (n_start + tl.arange(0, BLOCK_N)) % N  # size: (block_n)\n",
        "    offs_k = tl.arange(0, BLOCK_K)  # size: (block_k)\n",
        "    a_ptrs = a_ptr + (offs_m[:, None]*stride_am + offs_k[None, :]*stride_ak)  # size: (block_m, block_k)\n",
        "    b_ptrs = b_ptr + (offs_k[:, None]*stride_bk + offs_n[None, :]*stride_bn)  # size: (block_k, block_n)\n",
        "\n",
        "    for k in range(0, K, BLOCK_K):\n",
        "      a = tl.load(a_ptrs)\n",
        "      b = tl.load(b_ptrs)\n",
        "\n",
        "      # acc += tl.cast(tl.dot(a, b), tl.float16)\n",
        "      acc = tl.dot(a, b, acc, out_dtype=tl.float16)\n",
        "\n",
        "      a_ptrs += BLOCK_K*stride_ak\n",
        "      b_ptrs += BLOCK_K*stride_bk\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Apply ReLU and Add C to the accumulator\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    c_ptrs = c_ptr + (offs_m[:, None]*stride_cm + offs_n[None, :]*stride_cn)\n",
        "    c = tl.load(c_ptrs)\n",
        "    acc += c\n",
        "\n",
        "    acc = tl.where(acc >= 0, acc, 0)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "\n",
        "    d_ptrs = d_ptr + (offs_m[:, None]*stride_dm + offs_n[None, :]*stride_dn)\n",
        "\n",
        "    tl.store(d_ptrs, acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u16sz-IUuxNP"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_add_relu_fp16_adjustable_block_sizes(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, block_size_m, block_size_n, block_size_k) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ],
      "metadata": {
        "id": "vmYpwSZVLR9W"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AJ7LlTPawPqB"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "B4J5ZBpOuxNP",
        "outputId": "c12a4c83-928e-43ff-e7c0-34e27e1aa2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0625,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6094,  ..., 11.4609,  5.3750, 18.6250],\n",
            "        [ 2.7246,  0.0000,  0.0000,  ...,  0.0000, 26.0781,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4448, 75.1875,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9492,  1.1230,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6094, 26.9531, 22.9219,  ..., 13.5391,  6.0508, 21.6250]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
            "        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Accuracy Tests\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    # a = torch.tensor([[0.0]], device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    # b = torch.tensor([[0.0]], device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    # c = torch.tensor([[0.0]], device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kj_dGOlazQJY",
        "outputId": "8869a11c-0d67-4f84-e440-07af20cda60d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.02 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.03x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "K9Hdpxic0tq6"
      },
      "outputs": [],
      "source": [
        "# Write your grid search here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def triton_perf_benchmark(A, B, C, block_size_m, block_size_n, block_size_k, torch_time, repeats):\n",
        "  M = 2048\n",
        "  K = 2048\n",
        "  N = 2048\n",
        "\n",
        "  # warmup\n",
        "  _ = matmul_add_relu_fp16_adjustable_block_sizes(A, B, C, block_size_m, block_size_n, block_size_k)\n",
        "\n",
        "  # time your implementation\n",
        "  torch.cuda.synchronize()\n",
        "  start = time.perf_counter()\n",
        "  for _ in range(repeats):\n",
        "      _ = matmul_add_relu_fp16_adjustable_block_sizes(A, B, C, block_size_m, block_size_n, block_size_k)\n",
        "  torch.cuda.synchronize()\n",
        "  triton_time = (time.perf_counter() - start) / repeats\n",
        "\n",
        "  print(f'Block sizes: {block_size_m}, {block_size_n}, {block_size_k} --> {triton_time*1000:.2f} ms, speedup: {torch_time/triton_time:.2f}x')\n",
        "\n",
        "  return triton_time"
      ],
      "metadata": {
        "id": "urJAyoArKykv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "repeats = 2000\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "experiments = []\n",
        "\n",
        "for block_size_m in [32, 128, 256, 384, 512]:\n",
        "  for block_size_n in [32, 128, 256, 384, 512]:\n",
        "    for block_size_k in [16, 32, 64, 128]:\n",
        "      exp_time = triton_perf_benchmark(A, B, C, block_size_m, block_size_n, block_size_k, torch_time, repeats)\n",
        "      experiments.append({\n",
        "          \"time\": exp_time,\n",
        "          \"block_size_m\": block_size_m,\n",
        "          \"block_size_n\": block_size_n,\n",
        "          \"block_size_k\": block_size_k\n",
        "      })\n",
        "\n",
        "\n",
        "\n",
        "print(experiments)"
      ],
      "metadata": {
        "id": "19_8NEd6MX-R",
        "outputId": "c4c4991a-f4cd-4c8a-c820-2b34537f7eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch implementation\n",
            "Block sizes: 32, 32, 16 --> 1.01 ms, speedup: 0.98x\n",
            "Block sizes: 32, 32, 32 --> 1.02 ms, speedup: 0.98x\n",
            "Block sizes: 32, 32, 64 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 32, 32, 128 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 32, 128, 16 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 32, 128, 32 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 32, 128, 64 --> 1.06 ms, speedup: 0.94x\n",
            "Block sizes: 32, 128, 128 --> 1.07 ms, speedup: 0.93x\n",
            "Block sizes: 32, 256, 16 --> 1.08 ms, speedup: 0.92x\n",
            "Block sizes: 32, 256, 32 --> 1.08 ms, speedup: 0.92x\n",
            "Block sizes: 32, 256, 64 --> 1.09 ms, speedup: 0.91x\n",
            "Block sizes: 32, 256, 128 --> 1.10 ms, speedup: 0.90x\n",
            "Block sizes: 32, 384, 16 --> 1.10 ms, speedup: 0.90x\n",
            "Block sizes: 32, 384, 32 --> 1.10 ms, speedup: 0.91x\n",
            "Block sizes: 32, 384, 64 --> 1.09 ms, speedup: 0.91x\n",
            "Block sizes: 32, 384, 128 --> 1.08 ms, speedup: 0.92x\n",
            "Block sizes: 32, 512, 16 --> 1.07 ms, speedup: 0.92x\n",
            "Block sizes: 32, 512, 32 --> 1.07 ms, speedup: 0.93x\n",
            "Block sizes: 32, 512, 64 --> 1.06 ms, speedup: 0.94x\n",
            "Block sizes: 32, 512, 128 --> 1.06 ms, speedup: 0.94x\n",
            "Block sizes: 128, 32, 16 --> 1.05 ms, speedup: 0.94x\n",
            "Block sizes: 128, 32, 32 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 128, 32, 64 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 128, 32, 128 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 128, 128, 16 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 128, 128, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 128, 128, 64 --> 1.03 ms, speedup: 0.97x\n",
            "Block sizes: 128, 128, 128 --> 1.03 ms, speedup: 0.97x\n",
            "Block sizes: 128, 256, 16 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 256, 32 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 256, 64 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 256, 128 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 384, 16 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 384, 32 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 384, 64 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 384, 128 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 512, 16 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 512, 32 --> 1.02 ms, speedup: 0.97x\n",
            "Block sizes: 128, 512, 64 --> 1.03 ms, speedup: 0.97x\n",
            "Block sizes: 128, 512, 128 --> 1.03 ms, speedup: 0.97x\n",
            "Block sizes: 256, 32, 16 --> 1.03 ms, speedup: 0.97x\n",
            "Block sizes: 256, 32, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 256, 32, 64 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 256, 32, 128 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 256, 128, 16 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 256, 128, 32 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 256, 128, 64 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 256, 128, 128 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 256, 256, 16 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 256, 256, 32 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 256, 64 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 256, 128 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 384, 16 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 384, 32 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 384, 64 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 384, 128 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 512, 16 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 512, 32 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 512, 64 --> 1.05 ms, speedup: 0.95x\n",
            "Block sizes: 256, 512, 128 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 384, 32, 16 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 384, 32, 32 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 384, 32, 64 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 384, 32, 128 --> 1.04 ms, speedup: 0.95x\n",
            "Block sizes: 384, 128, 16 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 128, 32 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 128, 64 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 128, 128 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 256, 16 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 256, 32 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 256, 64 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 384, 256, 128 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 384, 16 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 384, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 384, 64 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 384, 128 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 512, 16 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 512, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 512, 64 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 384, 512, 128 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 32, 16 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 32, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 32, 64 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 32, 128 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 128, 16 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 128, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 128, 64 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 128, 128 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 256, 16 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 256, 32 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 256, 64 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 256, 128 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 384, 16 --> 1.03 ms, speedup: 0.96x\n",
            "Block sizes: 512, 384, 32 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 384, 64 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 384, 128 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 512, 16 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 512, 32 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 512, 64 --> 1.04 ms, speedup: 0.96x\n",
            "Block sizes: 512, 512, 128 --> 1.04 ms, speedup: 0.96x\n",
            "[{'time': 0.001009936016499978, 'block_size_m': 32, 'block_size_n': 32, 'block_size_k': 16}, {'time': 0.0010152048850000028, 'block_size_m': 32, 'block_size_n': 32, 'block_size_k': 32}, {'time': 0.0010244526655001209, 'block_size_m': 32, 'block_size_n': 32, 'block_size_k': 64}, {'time': 0.0010318325414998527, 'block_size_m': 32, 'block_size_n': 32, 'block_size_k': 128}, {'time': 0.0010426515115000256, 'block_size_m': 32, 'block_size_n': 128, 'block_size_k': 16}, {'time': 0.0010505482850001044, 'block_size_m': 32, 'block_size_n': 128, 'block_size_k': 32}, {'time': 0.0010597211075000813, 'block_size_m': 32, 'block_size_n': 128, 'block_size_k': 64}, {'time': 0.001067350790500086, 'block_size_m': 32, 'block_size_n': 128, 'block_size_k': 128}, {'time': 0.0010757144705000883, 'block_size_m': 32, 'block_size_n': 256, 'block_size_k': 16}, {'time': 0.0010844412389999433, 'block_size_m': 32, 'block_size_n': 256, 'block_size_k': 32}, {'time': 0.0010919254474999888, 'block_size_m': 32, 'block_size_n': 256, 'block_size_k': 64}, {'time': 0.0010979600914999992, 'block_size_m': 32, 'block_size_n': 256, 'block_size_k': 128}, {'time': 0.0011005617275000076, 'block_size_m': 32, 'block_size_n': 384, 'block_size_k': 16}, {'time': 0.001096060299499868, 'block_size_m': 32, 'block_size_n': 384, 'block_size_k': 32}, {'time': 0.0010904053149999981, 'block_size_m': 32, 'block_size_n': 384, 'block_size_k': 64}, {'time': 0.001080651509999825, 'block_size_m': 32, 'block_size_n': 384, 'block_size_k': 128}, {'time': 0.001073452742499967, 'block_size_m': 32, 'block_size_n': 512, 'block_size_k': 16}, {'time': 0.0010657458969999425, 'block_size_m': 32, 'block_size_n': 512, 'block_size_k': 32}, {'time': 0.0010618735745001686, 'block_size_m': 32, 'block_size_n': 512, 'block_size_k': 64}, {'time': 0.0010565178599999854, 'block_size_m': 32, 'block_size_n': 512, 'block_size_k': 128}, {'time': 0.0010538983505000489, 'block_size_m': 128, 'block_size_n': 32, 'block_size_k': 16}, {'time': 0.001047517843499918, 'block_size_m': 128, 'block_size_n': 32, 'block_size_k': 32}, {'time': 0.0010421777830001702, 'block_size_m': 128, 'block_size_n': 32, 'block_size_k': 64}, {'time': 0.0010371838855000987, 'block_size_m': 128, 'block_size_n': 32, 'block_size_k': 128}, {'time': 0.0010356874374999733, 'block_size_m': 128, 'block_size_n': 128, 'block_size_k': 16}, {'time': 0.001030885506000004, 'block_size_m': 128, 'block_size_n': 128, 'block_size_k': 32}, {'time': 0.001028842854000004, 'block_size_m': 128, 'block_size_n': 128, 'block_size_k': 64}, {'time': 0.00102571049449989, 'block_size_m': 128, 'block_size_n': 128, 'block_size_k': 128}, {'time': 0.0010237370525001098, 'block_size_m': 128, 'block_size_n': 256, 'block_size_k': 16}, {'time': 0.0010228304720001234, 'block_size_m': 128, 'block_size_n': 256, 'block_size_k': 32}, {'time': 0.0010215889554999649, 'block_size_m': 128, 'block_size_n': 256, 'block_size_k': 64}, {'time': 0.0010204374625000128, 'block_size_m': 128, 'block_size_n': 256, 'block_size_k': 128}, {'time': 0.0010197861755000303, 'block_size_m': 128, 'block_size_n': 384, 'block_size_k': 16}, {'time': 0.0010190361165000469, 'block_size_m': 128, 'block_size_n': 384, 'block_size_k': 32}, {'time': 0.0010193820255001355, 'block_size_m': 128, 'block_size_n': 384, 'block_size_k': 64}, {'time': 0.0010204667054999844, 'block_size_m': 128, 'block_size_n': 384, 'block_size_k': 128}, {'time': 0.0010218135840000286, 'block_size_m': 128, 'block_size_n': 512, 'block_size_k': 16}, {'time': 0.0010243182014999092, 'block_size_m': 128, 'block_size_n': 512, 'block_size_k': 32}, {'time': 0.001025231943000108, 'block_size_m': 128, 'block_size_n': 512, 'block_size_k': 64}, {'time': 0.0010270970315000342, 'block_size_m': 128, 'block_size_n': 512, 'block_size_k': 128}, {'time': 0.0010286922674999914, 'block_size_m': 256, 'block_size_n': 32, 'block_size_k': 16}, {'time': 0.0010318778379999002, 'block_size_m': 256, 'block_size_n': 32, 'block_size_k': 32}, {'time': 0.0010333525989999544, 'block_size_m': 256, 'block_size_n': 32, 'block_size_k': 64}, {'time': 0.001036189949000118, 'block_size_m': 256, 'block_size_n': 32, 'block_size_k': 128}, {'time': 0.0010366713700000218, 'block_size_m': 256, 'block_size_n': 128, 'block_size_k': 16}, {'time': 0.0010377210500000728, 'block_size_m': 256, 'block_size_n': 128, 'block_size_k': 32}, {'time': 0.0010412094724999862, 'block_size_m': 256, 'block_size_n': 128, 'block_size_k': 64}, {'time': 0.0010433115030000408, 'block_size_m': 256, 'block_size_n': 128, 'block_size_k': 128}, {'time': 0.0010448185180000564, 'block_size_m': 256, 'block_size_n': 256, 'block_size_k': 16}, {'time': 0.0010461572220001472, 'block_size_m': 256, 'block_size_n': 256, 'block_size_k': 32}, {'time': 0.0010483294445000411, 'block_size_m': 256, 'block_size_n': 256, 'block_size_k': 64}, {'time': 0.0010452529924998544, 'block_size_m': 256, 'block_size_n': 256, 'block_size_k': 128}, {'time': 0.0010481933649998608, 'block_size_m': 256, 'block_size_n': 384, 'block_size_k': 16}, {'time': 0.001050428346999979, 'block_size_m': 256, 'block_size_n': 384, 'block_size_k': 32}, {'time': 0.001048864030499999, 'block_size_m': 256, 'block_size_n': 384, 'block_size_k': 64}, {'time': 0.0010497924264998347, 'block_size_m': 256, 'block_size_n': 384, 'block_size_k': 128}, {'time': 0.001048067724500015, 'block_size_m': 256, 'block_size_n': 512, 'block_size_k': 16}, {'time': 0.0010471794469999623, 'block_size_m': 256, 'block_size_n': 512, 'block_size_k': 32}, {'time': 0.0010453408570001556, 'block_size_m': 256, 'block_size_n': 512, 'block_size_k': 64}, {'time': 0.0010443197335000605, 'block_size_m': 256, 'block_size_n': 512, 'block_size_k': 128}, {'time': 0.001042734527499988, 'block_size_m': 384, 'block_size_n': 32, 'block_size_k': 16}, {'time': 0.0010423409974998777, 'block_size_m': 384, 'block_size_n': 32, 'block_size_k': 32}, {'time': 0.001040492729999869, 'block_size_m': 384, 'block_size_n': 32, 'block_size_k': 64}, {'time': 0.0010398599979998834, 'block_size_m': 384, 'block_size_n': 32, 'block_size_k': 128}, {'time': 0.001039011303000052, 'block_size_m': 384, 'block_size_n': 128, 'block_size_k': 16}, {'time': 0.001039633250999941, 'block_size_m': 384, 'block_size_n': 128, 'block_size_k': 32}, {'time': 0.0010376909095000427, 'block_size_m': 384, 'block_size_n': 128, 'block_size_k': 64}, {'time': 0.001037216945999944, 'block_size_m': 384, 'block_size_n': 128, 'block_size_k': 128}, {'time': 0.0010358772834999854, 'block_size_m': 384, 'block_size_n': 256, 'block_size_k': 16}, {'time': 0.001036607965500025, 'block_size_m': 384, 'block_size_n': 256, 'block_size_k': 32}, {'time': 0.0010353355385000213, 'block_size_m': 384, 'block_size_n': 256, 'block_size_k': 64}, {'time': 0.0010341953454999385, 'block_size_m': 384, 'block_size_n': 256, 'block_size_k': 128}, {'time': 0.0010347193674999744, 'block_size_m': 384, 'block_size_n': 384, 'block_size_k': 16}, {'time': 0.001032827357500082, 'block_size_m': 384, 'block_size_n': 384, 'block_size_k': 32}, {'time': 0.001033030240500011, 'block_size_m': 384, 'block_size_n': 384, 'block_size_k': 64}, {'time': 0.001033115466499794, 'block_size_m': 384, 'block_size_n': 384, 'block_size_k': 128}, {'time': 0.0010322270135000053, 'block_size_m': 384, 'block_size_n': 512, 'block_size_k': 16}, {'time': 0.001032229995499847, 'block_size_m': 384, 'block_size_n': 512, 'block_size_k': 32}, {'time': 0.001031528944999991, 'block_size_m': 384, 'block_size_n': 512, 'block_size_k': 64}, {'time': 0.0010319959514999938, 'block_size_m': 384, 'block_size_n': 512, 'block_size_k': 128}, {'time': 0.0010324132265000117, 'block_size_m': 512, 'block_size_n': 32, 'block_size_k': 16}, {'time': 0.0010317844420001166, 'block_size_m': 512, 'block_size_n': 32, 'block_size_k': 32}, {'time': 0.0010339391339998657, 'block_size_m': 512, 'block_size_n': 32, 'block_size_k': 64}, {'time': 0.0010329406849998576, 'block_size_m': 512, 'block_size_n': 32, 'block_size_k': 128}, {'time': 0.0010326058740001826, 'block_size_m': 512, 'block_size_n': 128, 'block_size_k': 16}, {'time': 0.001032982995500106, 'block_size_m': 512, 'block_size_n': 128, 'block_size_k': 32}, {'time': 0.0010338403445000494, 'block_size_m': 512, 'block_size_n': 128, 'block_size_k': 64}, {'time': 0.0010347715940001762, 'block_size_m': 512, 'block_size_n': 128, 'block_size_k': 128}, {'time': 0.0010344887179999204, 'block_size_m': 512, 'block_size_n': 256, 'block_size_k': 16}, {'time': 0.0010340215490000446, 'block_size_m': 512, 'block_size_n': 256, 'block_size_k': 32}, {'time': 0.0010354991080000673, 'block_size_m': 512, 'block_size_n': 256, 'block_size_k': 64}, {'time': 0.001035133844000029, 'block_size_m': 512, 'block_size_n': 256, 'block_size_k': 128}, {'time': 0.0010349133450001772, 'block_size_m': 512, 'block_size_n': 384, 'block_size_k': 16}, {'time': 0.001035826125000085, 'block_size_m': 512, 'block_size_n': 384, 'block_size_k': 32}, {'time': 0.00103602117000014, 'block_size_m': 512, 'block_size_n': 384, 'block_size_k': 64}, {'time': 0.0010369140090001564, 'block_size_m': 512, 'block_size_n': 384, 'block_size_k': 128}, {'time': 0.0010370327804998852, 'block_size_m': 512, 'block_size_n': 512, 'block_size_k': 16}, {'time': 0.0010380434485000479, 'block_size_m': 512, 'block_size_n': 512, 'block_size_k': 32}, {'time': 0.0010376643194999816, 'block_size_m': 512, 'block_size_n': 512, 'block_size_k': 64}, {'time': 0.0010378514194999298, 'block_size_m': 512, 'block_size_n': 512, 'block_size_k': 128}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1eskeATNxUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}